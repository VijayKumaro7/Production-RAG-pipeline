"""
ragas_eval.py â€” RAGAS evaluation pipeline for RAG systems.

Evaluates the RAG pipeline on 4 core metrics:
  - Faithfulness: Is the answer grounded in the context?
  - Answer Relevancy: Does the answer address the question?
  - Context Precision: Is the retrieved context relevant?
  - Context Recall: Does the context cover the ground truth?

Usage:
    python evaluation/ragas_eval.py --config config.yaml
"""

import json
import os
import sys
import argparse
from datetime import datetime
from typing import Any, Dict, List, Optional

import yaml
import pandas as pd
from loguru import logger
from dotenv import load_dotenv
from datasets import Dataset

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

load_dotenv()


def load_test_questions(path: str) -> List[Dict]:
    """
    Load the golden QA dataset from JSON.

    Args:
        path: Path to test_questions.json.

    Returns:
        List of question dicts with keys: question, ground_truth, contexts, answer.
    """
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    logger.info(f"Loaded {len(data)} test questions from {path}")
    return data


def run_pipeline_on_questions(
    questions: List[Dict],
    pipeline,
) -> List[Dict]:
    """
    Run the RAG pipeline on all test questions and collect results.

    Args:
        questions: List of test question dicts.
        pipeline: Initialized RAGPipeline instance.

    Returns:
        Updated list with 'answer' and 'contexts' filled in by the pipeline.
    """
    results = []
    total = len(questions)

    for i, item in enumerate(questions, 1):
        question = item["question"]
        logger.info(f"Processing [{i}/{total}]: {question[:60]}...")

        try:
            response = pipeline.query(question)

            results.append({
                "question": question,
                "answer": response.answer,
                "contexts": [doc.page_content for doc in response.source_documents],
                "ground_truth": item.get("ground_truth", ""),
            })
        except Exception as e:
            logger.error(f"Failed to process question '{question}': {e}")
            results.append({
                "question": question,
                "answer": "ERROR: " + str(e),
                "contexts": [],
                "ground_truth": item.get("ground_truth", ""),
            })

    return results


def evaluate_with_ragas(
    eval_data: List[Dict],
    metrics: Optional[List] = None,
    llm=None,
    embeddings=None,
) -> Dict[str, float]:
    """
    Run RAGAS evaluation on a list of QA samples.

    Args:
        eval_data: List of dicts with keys: question, answer, contexts, ground_truth.
        metrics: List of RAGAS metric objects. Defaults to all 4 core metrics.
        llm: Optional LLM for RAGAS (uses OpenAI by default).
        embeddings: Optional embeddings for RAGAS answer relevancy.

    Returns:
        Dict mapping metric name â†’ score (0.0 to 1.0).
    """
    try:
        from ragas import evaluate
        from ragas.metrics import (
            faithfulness,
            answer_relevancy,
            context_recall,
            context_precision,
        )
    except ImportError:
        logger.error("RAGAS not installed. Run: pip install ragas")
        raise

    if metrics is None:
        metrics = [faithfulness, answer_relevancy, context_recall, context_precision]

    # Build HuggingFace Dataset
    dataset = Dataset.from_dict({
        "question": [d["question"] for d in eval_data],
        "answer": [d["answer"] for d in eval_data],
        "contexts": [d["contexts"] for d in eval_data],
        "ground_truth": [d["ground_truth"] for d in eval_data],
    })

    logger.info(f"Running RAGAS evaluation on {len(eval_data)} samples...")
    logger.info(f"Metrics: {[m.name for m in metrics]}")

    try:
        # RAGAS uses OpenAI by default â€” can be overridden
        result = evaluate(
            dataset=dataset,
            metrics=metrics,
            llm=llm,
            embeddings=embeddings,
            raise_exceptions=False,
        )

        scores = {}
        for metric in metrics:
            metric_name = metric.name
            if metric_name in result:
                scores[metric_name] = float(result[metric_name])
            else:
                scores[metric_name] = None
                logger.warning(f"Metric '{metric_name}' not found in results")

        logger.info(f"RAGAS Scores: {scores}")
        return scores

    except Exception as e:
        logger.error(f"RAGAS evaluation failed: {e}")
        raise


def generate_evaluation_report(
    scores: Dict[str, float],
    config: Dict[str, Any],
    num_samples: int,
    output_path: str = None,
) -> Dict[str, Any]:
    """
    Generate a structured evaluation report JSON.

    Args:
        scores: Dict of metric name â†’ score.
        config: Pipeline configuration dict.
        num_samples: Number of evaluated samples.
        output_path: Where to save the report. If None, uses timestamped path.

    Returns:
        Report dict saved to disk.
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    report = {
        "timestamp": timestamp,
        "num_samples": num_samples,
        "config": config,
        "scores": scores,
        "aggregate": {
            "mean_score": (
                sum(v for v in scores.values() if v is not None)
                / max(1, sum(1 for v in scores.values() if v is not None))
            ),
            "min_score": min((v for v in scores.values() if v is not None), default=0),
            "max_score": max((v for v in scores.values() if v is not None), default=0),
        },
    }

    if output_path is None:
        os.makedirs("experiments/results", exist_ok=True)
        output_path = f"experiments/results/eval_{timestamp}.json"

    with open(output_path, "w") as f:
        json.dump(report, f, indent=2)

    logger.info(f"Evaluation report saved to {output_path}")

    # Print summary table
    print("\n" + "=" * 55)
    print("ðŸ“Š RAGAS Evaluation Report")
    print("=" * 55)
    print(f"{'Metric':<25} {'Score':>10}")
    print("-" * 35)
    for metric, score in scores.items():
        score_str = f"{score:.4f}" if score is not None else "N/A"
        print(f"{metric:<25} {score_str:>10}")
    print("-" * 35)
    print(f"{'Mean Score':<25} {report['aggregate']['mean_score']:.4f}")
    print("=" * 55)

    return report


def run_full_evaluation(
    pipeline,
    config: Dict[str, Any],
    test_questions_path: str = "evaluation/test_questions.json",
    output_dir: str = "experiments/results",
) -> Dict[str, Any]:
    """
    Full evaluation workflow: load questions â†’ run pipeline â†’ RAGAS eval â†’ report.

    Args:
        pipeline: Initialized RAGPipeline.
        config: Config dict for logging.
        test_questions_path: Path to test_questions.json.
        output_dir: Directory to save results.

    Returns:
        Evaluation report dict.
    """
    logger.info("=== Starting Full RAGAS Evaluation ===")

    # Load test questions
    questions = load_test_questions(test_questions_path)

    # Run pipeline on all questions
    eval_data = run_pipeline_on_questions(questions, pipeline)

    # Run RAGAS
    scores = evaluate_with_ragas(eval_data)

    # Generate report
    os.makedirs(output_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = os.path.join(output_dir, f"eval_{timestamp}.json")
    report = generate_evaluation_report(scores, config, len(eval_data), output_path)

    logger.info("=== Evaluation Complete ===")
    return report


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run RAGAS evaluation on RAG pipeline")
    parser.add_argument("--config", default="config.yaml", help="Path to config.yaml")
    parser.add_argument(
        "--questions", default="evaluation/test_questions.json", help="Path to test questions"
    )
    args = parser.parse_args()

    from src.pipeline import create_pipeline_from_yaml

    pipeline = create_pipeline_from_yaml(config_path=args.config)
    pipeline.initialize(skip_ingestion=True)

    with open(args.config) as f:
        config = yaml.safe_load(f)

    report = run_full_evaluation(pipeline, config, args.questions)
    print(f"\nâœ… Report saved. Mean score: {report['aggregate']['mean_score']:.4f}")
